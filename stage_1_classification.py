import matplotlib
# –í–º–∏–∫–∞—î–º–æ "–±–µ–∑–æ–∫–æ–Ω–Ω–∏–π" —Ä–µ–∂–∏–º –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä—ñ/–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ
matplotlib.use('Agg') 

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from facenet_pytorch import MTCNN
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import zipfile

# --- –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ---
BATCH_SIZE = 16
EPOCHS = 10
LEARNING_RATE = 0.001
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# –®–ª—è—Ö–∏
DATA_ROOT = './Data'
LFW_ZIP_PATH = os.path.join(DATA_ROOT, 'LFW', 'archive.zip') 
EXTRACT_DIR = os.path.join(DATA_ROOT, 'LFW_Extracted')        
LOCAL_IMAGES_DIR = os.path.join(DATA_ROOT, 'Faces')
MODELS_DIR = './models'
RESULTS_DIR = './results'

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(EXTRACT_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {DEVICE}")

# --- 0. –†–û–ó–ü–ê–ö–£–í–ê–ù–ù–Ø ZIP ---

def extract_lfw_zip():
    if os.listdir(EXTRACT_DIR):
        print("‚úÖ –ü–∞–ø–∫–∞ —Ä–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –Ω–µ –ø–æ—Ä–æ–∂–Ω—è. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è.")
        return

    if not os.path.exists(LFW_ZIP_PATH):
        print(f"‚ùå –§–∞–π–ª –∞—Ä—Ö—ñ–≤—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {LFW_ZIP_PATH}")
        exit()

    print(f"‚è≥ –†–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è {LFW_ZIP_PATH}...")
    try:
        with zipfile.ZipFile(LFW_ZIP_PATH, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_DIR)
        print("‚úÖ –†–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ä–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è: {e}")
        exit()

extract_lfw_zip()

# --- 1. –ü–û–®–£–ö –ö–û–†–ï–ù–Ø –î–ê–¢–ê–°–ï–¢–£ ---

def find_dataset_root(base_dir):
    target = "Gerhard" 
    for root, dirs, files in os.walk(base_dir):
        for d in dirs:
            if d.startswith(target):
                return root 
    return base_dir

print("üîç –ü–æ—à—É–∫ –ø–∞–ø–∫–∏ –∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏...")
REAL_LFW_ROOT = find_dataset_root(EXTRACT_DIR)
print(f"üìÇ –ö–æ—Ä—ñ–Ω—å –¥–∞—Ç–∞—Å–µ—Ç—É –∑–Ω–∞–π–¥–µ–Ω–æ: {REAL_LFW_ROOT}")

# --- 2. –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• ---

train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

full_dataset = datasets.ImageFolder(root=REAL_LFW_ROOT, transform=train_transforms)

possible_targets = [
    ["Gerhard_Schroeder", "Tony_Blair", "Colin_Powell", "George_W_Bush", "Donald_Rumsfeld"],
    ["Gerhard Schroeder", "Tony Blair", "Colin Powell", "George W Bush", "Donald Rumsfeld"]
]

target_classes = []
class_to_idx = full_dataset.class_to_idx

for candidates in possible_targets:
    if candidates[0] in class_to_idx:
        target_classes = candidates
        break

if not target_classes:
    print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ü—ñ–ª—å–æ–≤–∏—Ö –∫–ª–∞—Å—ñ–≤!")
    exit()

print(f"‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –∫–ª–∞—Å–∏: {target_classes}")

target_indices = [class_to_idx[name] for name in target_classes]
model_idx_to_name = {i: name.replace('_', ' ') for i, name in enumerate(target_classes)}

indices = []
for i, label in enumerate(full_dataset.targets):
    if label in target_indices:
        indices.append(i)

class FilteredDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, original_target_indices):
        self.dataset = dataset
        self.indices = indices
        self.map_label = {old: new for new, old in enumerate(original_target_indices)}

    def __getitem__(self, idx):
        img, label = self.dataset[self.indices[idx]]
        return img, self.map_label[label]

    def __len__(self):
        return len(self.indices)

train_subset = FilteredDataset(full_dataset, indices, target_indices)
train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

print(f"–î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤–∏–π: {len(train_subset)} —Ñ–æ—Ç–æ.")

# --- 3. –ê–†–•–Ü–¢–ï–ö–¢–£–†–ò ---

class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 28 * 28, 512),
            nn.ReLU(),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.classifier(self.features(x))

def get_resnet_architecture(num_classes):
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    for param in model.parameters():
        param.requires_grad = False
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    return model

# --- 4. –¢–†–ï–ù–£–í–ê–ù–ù–Ø ---

def get_or_train_model(model, name, filename):
    save_path = os.path.join(MODELS_DIR, filename)
    model = model.to(DEVICE)
    
    if os.path.exists(save_path):
        print(f"\n[{name}] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
        model.load_state_dict(torch.load(save_path, map_location=DEVICE))
        return model
    
    print(f"\n[{name}] –ü–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ CPU...")
    start_time = time.time()
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss/len(train_loader):.4f} | Acc: {100 * correct / total:.1f}%")
    
    duration = time.time() - start_time
    print(f"[{name}] –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.1f} —Å–µ–∫. –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {save_path}")
    torch.save(model.state_dict(), save_path)
    return model

# --- –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø ---

num_classes = len(target_classes)

simple_net = SimpleCNN(num_classes)
simple_model = get_or_train_model(simple_net, "SimpleCNN", "simple_cnn_lfw_manual.pth")

resnet_net = get_resnet_architecture(num_classes)
resnet_model = get_or_train_model(resnet_net, "ResNet50", "resnet50_lfw_manual.pth")

# --- 5. –Ü–ù–§–ï–†–ï–ù–° –¢–ê –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø (PyTorch MTCNN) ---

detector = MTCNN(keep_all=False, select_largest=True, device=DEVICE)

def predict_local_image(model, model_name, image_path):
    if not os.path.exists(image_path):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {image_path}")
        return None, None, None

    try:
        pil_img = Image.open(image_path).convert('RGB')
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è: {e}")
        return None, None, None
    
    boxes, probs = detector.detect(pil_img)
    
    if boxes is None:
        print(f"[{model_name}] –û–±–ª–∏—á—á—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ {os.path.basename(image_path)}")
        return None, None, None

    box = boxes[0]
    x1, y1, x2, y2 = [int(b) for b in box]
    x1, y1 = max(0, x1), max(0, y1)
    
    face_img = pil_img.crop((x1, y1, x2, y2))
    face_img_resized = face_img.resize((224, 224))
    
    input_tensor = train_transforms(face_img_resized).unsqueeze(0).to(DEVICE)
    
    model.eval()
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        top_prob, top_catid = torch.max(probabilities, 1)
        
    predicted_label = model_idx_to_name[top_catid.item()]
    confidence = top_prob.item() * 100
    
    print(f"–ú–æ–¥–µ–ª—å: {model_name:<10} | –§–∞–π–ª: {os.path.basename(image_path):<15} -> {predicted_label} ({confidence:.2f}%)")
    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    return face_img_resized, predicted_label, confidence

# –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —É —Ñ–∞–π–ª
def save_result_image(img, pred, conf, original_filename, model_name):
    if img is None: return
    
    plt.figure(figsize=(3, 3))
    plt.imshow(img)
    # –ó–µ–ª–µ–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, —è–∫—â–æ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å > 70%, —ñ–Ω–∞–∫—à–µ —á–µ—Ä–≤–æ–Ω–∏–π
    title_color = "green" if conf > 70 else "red"
    # –î–æ–¥–∞—î–º–æ –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ –≤ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    plt.title(f"{model_name}\n{pred}\n{conf:.1f}%", color=title_color, fontsize=10)
    plt.axis('off')
    
    # –§–æ—Ä–º—É—î–º–æ —ñ–º'—è —Ñ–∞–π–ª—É: result_originalName_ModelName.png
    base_name = os.path.basename(original_filename).split('.')[0]
    save_name = f"result_{base_name}_{model_name.replace(' ', '')}.png"
    
    plt.savefig(RESULTS_DIR+'/'+save_name, bbox_inches='tight')
    print(f"üñºÔ∏è –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {save_name}")
    plt.close() # –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ –æ–±–æ–≤'—è–∑–∫–æ–≤–µ

print("\n=== –ï–¢–ê–ü 1: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏—Ö —Ñ–æ—Ç–æ ===")
test_files = [f for f in os.listdir(LOCAL_IMAGES_DIR) if f.lower().endswith(('.jpg', '.png'))]
test_files.sort()

if not test_files:
    print(f"–£ –ø–∞–ø—Ü—ñ {LOCAL_IMAGES_DIR} –Ω–µ–º–∞—î –∑–æ–±—Ä–∞–∂–µ–Ω—å! –î–æ–¥–∞–π—Ç–µ —Ñ–æ—Ç–æ (schroeder_1.jpg, abbey_1.jpg).")
else:
    for file_name in test_files:
        full_path = os.path.join(LOCAL_IMAGES_DIR, file_name)
        print("-" * 60)
        print(f"–û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file_name}")
        
        # 1. –¢–µ—Å—Ç SimpleCNN
        img_s, pred_s, conf_s = predict_local_image(simple_model, "SimpleCNN", full_path)
        save_result_image(img_s, pred_s, conf_s, file_name, "SimpleCNN")
        
        # 2. –¢–µ—Å—Ç ResNet50
        img_r, pred_r, conf_r = predict_local_image(resnet_model, "ResNet50", full_path)
        save_result_image(img_r, pred_r, conf_r, file_name, "ResNet50")

print("\n–í–∏—Å–Ω–æ–≤–∫–∏ –¥–ª—è –î–∏–ø–ª–æ–º—É (–ï—Ç–∞–ø 1):")
print("–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –í–∏ –ø–æ–±–∞—á–∏—Ç–µ, —â–æ –¥–ª—è Jeff –æ–±–∏–¥–≤—ñ –º–æ–¥–µ–ª—ñ")
print("–≤–∏–¥–∞—é—Ç—å —Ö–∏–±–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ –≤–∏—Å–æ–∫–æ—é –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é (–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—è).")