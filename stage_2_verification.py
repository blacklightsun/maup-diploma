import matplotlib
# –í–º–∏–∫–∞—î–º–æ —Ä–µ–∂–∏–º –±–µ–∑ GUI
matplotlib.use('Agg') 

import torch
import torch.nn.functional as F
from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import shutil

# --- –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 

# –®–ª—è—Ö–∏ (–º–∞—é—Ç—å –∑–±—ñ–≥–∞—Ç–∏—Å—è –∑ –ï—Ç–∞–ø–æ–º 1)
DATA_ROOT = './Data'
RESULTS_DIR = './results_stage2'
LFW_EXTRACTED = os.path.join(DATA_ROOT, 'LFW_Extracted') # –¢—É—Ç –ª–µ–∂–∏—Ç—å —Ä–æ–∑–ø–∞–∫–æ–≤–∞–Ω–∏–π LFW
LOCAL_IMAGES_DIR = os.path.join(DATA_ROOT, 'Faces')      # –í–∞—à—ñ —Ç–µ—Å—Ç–æ–≤—ñ —Ñ–æ—Ç–æ

os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {DEVICE}")

# --- 1. –ü–û–®–£–ö –ï–¢–ê–õ–û–ù–£ (ANCHOR) –í –î–ê–¢–ê–°–ï–¢–Ü ---

# –ó–ú–Ü–ù–Ü–¢–¨ –¶–ï –ß–ò–°–õ–û, —â–æ–± –≤–∏–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ —Ñ–æ—Ç–æ
# 0 - –ø–µ—Ä—à–µ —Ñ–æ—Ç–æ, 1 - –¥—Ä—É–≥–µ, —ñ —Ç.–¥.
# –†–µ–∫–æ–º–µ–Ω–¥—É—é —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ 1, 2 –∞–±–æ 4 (–∑–∞–∑–≤–∏—á–∞–π —Ç–∞–º –∫—Ä–∞—â—ñ —Ä–∞–∫—É—Ä—Å–∏)
ANCHOR_INDEX = 1 

def get_anchor_image_path(desired_index):
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å —Ñ–æ—Ç–æ –ì–µ—Ä—Ö–∞—Ä–¥–∞ –®—Ä—å–æ–¥–µ—Ä–∞ –∑–∞ –≤–∫–∞–∑–∞–Ω–∏–º —ñ–Ω–¥–µ–∫—Å–æ–º"""
    target_name = "Gerhard_Schroeder"
    candidate_files = []
    
    # 1. –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ñ–æ—Ç–æ
    for root, dirs, files in os.walk(LFW_EXTRACTED):
        if os.path.basename(root).startswith(target_name):
            images = [f for f in files if f.lower().endswith(('.jpg', '.jpeg'))]
            # –°–æ—Ä—Ç—É—î–º–æ, —â–æ–± –ø–æ—Ä—è–¥–æ–∫ –±—É–≤ –∑–∞–≤–∂–¥–∏ –æ–¥–Ω–∞–∫–æ–≤–∏–º
            images.sort()
            
            for img in images:
                candidate_files.append(os.path.join(root, img))
    
    if not candidate_files:
        return None

    print(f"\n–ó–Ω–∞–π–¥–µ–Ω–æ {len(candidate_files)} —Ñ–æ—Ç–æ –¥–ª—è –µ—Ç–∞–ª–æ–Ω—É:")
    for i, path in enumerate(candidate_files):
        # –í–∏–≤–æ–¥–∏–º–æ –ø–µ—Ä—à—ñ 5 —ñ –≤–∏–±—Ä–∞–Ω–µ
        if i < 5 or i == desired_index:
            marker = "  <--- –í–ò–ë–†–ê–ù–û" if i == desired_index else ""
            print(f"  [{i}]: {os.path.basename(path)}{marker}")
    
    # 2. –í–∏–±–∏—Ä–∞—î–º–æ –ø–æ—Ç—Ä—ñ–±–Ω–µ
    if 0 <= desired_index < len(candidate_files):
        return candidate_files[desired_index]
    else:
        print(f"‚ö†Ô∏è –Ü–Ω–¥–µ–∫—Å {desired_index} –≤–∏—Ö–æ–¥–∏—Ç—å –∑–∞ –º–µ–∂—ñ (–¥–æ—Å—Ç—É–ø–Ω–æ {len(candidate_files)}). –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—î.")
        return candidate_files[-1]

anchor_path = get_anchor_image_path(ANCHOR_INDEX)

if not anchor_path:
    print("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–∞–ø–∫—É –∑ –ì–µ—Ä—Ö–∞—Ä–¥–æ–º –®—Ä—å–æ–¥–µ—Ä–æ–º.")
    exit()

print(f"‚úÖ –ï—Ç–∞–ª–æ–Ω (Anchor) —É—Å–ø—ñ—à–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {os.path.basename(anchor_path)}")



# --- 2. –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø –Ü–ù–°–¢–†–£–ú–ï–ù–¢–Ü–í ---

# –î–µ—Ç–µ–∫—Ç–æ—Ä
mtcnn = MTCNN(keep_all=False, select_largest=True, device=DEVICE)

# –ú–æ–¥–µ–ª—å –ê: –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –ø—Ä–æ–±–ª–µ–º–∏)
model_classifier = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=8631).to(DEVICE)
model_classifier.eval()

# –ú–æ–¥–µ–ª—å –ë: –ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä (–¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º–∏)
model_extractor = InceptionResnetV1(pretrained='vggface2', classify=False).to(DEVICE)
model_extractor.eval()

# --- 3. –ß–ê–°–¢–ò–ù–ê –ê: –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø –ü–†–û–ë–õ–ï–ú–ò –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–á ---

def demo_classification_failure(image_path, model):
    try:
        img = Image.open(image_path).convert('RGB')
    except:
        return

    # –î–µ—Ç–µ–∫—Ü—ñ—è —Ç–∞ –∫—Ä–æ–ø
    x_aligned, prob = mtcnn(img, return_prob=True)
    
    if x_aligned is not None:
        # –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å
        # MTCNN –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–Ω–∑–æ—Ä –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —è–∫ [-1, 1], —Ü–µ –æ–∫ –¥–ª—è InceptionResnetV1
        input_tensor = x_aligned.unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            logits = model(input_tensor)
            probs = F.softmax(logits, dim=1)
            top_prob, top_class = torch.max(probs, 1)
            
        conf = top_prob.item() * 100
        
        print(f"[Classify=True]  –§–∞–π–ª: {os.path.basename(image_path):<15} -> ClassID: {top_class.item()} | Conf: {conf:.4f}%")
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        plt.figure(figsize=(4, 4))
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–Ω–∑–æ—Ä –Ω–∞–∑–∞–¥ –≤ –∫–∞—Ä—Ç–∏–Ω–∫—É –¥–ª—è –ø–æ–∫–∞–∑—É
        img_show = x_aligned.permute(1, 2, 0).cpu().numpy()
        img_show = (img_show * 128 + 127.5) / 255.0 # Denormalize
        img_show = np.clip(img_show, 0, 1)
        
        plt.imshow(img_show)
        plt.title(f"Raw Classifier Output:\nClass {top_class.item()} ({conf:.2f}%)", color="red")
        plt.axis('off')
        save_name = f"stage2_FAIL_classify_{os.path.basename(image_path).split('.')[0]}.png"
        plt.savefig(RESULTS_DIR+'/'+save_name)
        plt.close()

# --- 4. –ß–ê–°–¢–ò–ù–ê –ë: –†–Ü–®–ï–ù–ù–Ø –ß–ï–†–ï–ó –í–ï–†–ò–§–Ü–ö–ê–¶–Ü–Æ (–ú–ï–¢–†–ò–ö–ò) ---

def get_embedding(model, image_path):
    try:
        img = Image.open(image_path).convert('RGB')
    except:
        return None, None

    # MTCNN –ø–æ–≤–µ—Ä—Ç–∞—î –∫—Ä–æ–ø–Ω—É—Ç–µ –æ–±–ª–∏—á—á—è —è–∫ —Ç–µ–Ω–∑–æ—Ä
    x_aligned, prob = mtcnn(img, return_prob=True)
    
    if x_aligned is not None:
        input_tensor = x_aligned.unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            embedding = model(input_tensor)
            
        return embedding.cpu(), x_aligned
    return None, None

# –û—Ç—Ä–∏–º—É—î–º–æ –≤–µ–∫—Ç–æ—Ä –ï–¢–ê–õ–û–ù–£
print("\n--- –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä–∞ –ï—Ç–∞–ª–æ–Ω—É (Anchor) ---")
anchor_emb, anchor_img_tensor = get_embedding(model_extractor, anchor_path)

if anchor_emb is None:
    print("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ –µ—Ç–∞–ª–æ–Ω.")
    exit()

def run_verification(test_image_path):
    print(f"\nüîç –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è: {os.path.basename(test_image_path)}")
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –≤–µ–∫—Ç–æ—Ä –¢–ï–°–¢–£
    test_emb, test_img_tensor = get_embedding(model_extractor, test_image_path)
    
    if test_emb is None:
        print("–û–±–ª–∏—á—á—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    # 1. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—ñ–¥—Å—Ç–∞–Ω—ñ (L2 - Euclidean)
    dist = (anchor_emb - test_emb).norm().item()
    
    # –ü–æ—Ä—ñ–≥ (Threshold)
    # –î–ª—è VGGFace2 / L2 distance —Ö–æ—Ä–æ—à–∏–π –ø–æ—Ä—ñ–≥ –±–ª–∏–∑—å–∫–æ 1.0 - 1.1
    threshold = 1.0
    is_match = dist < threshold
    
    verdict = "–¶–ï –í–Ü–ù (MATCH)" if is_match else "–ß–£–ñ–ò–ô (MISMATCH)"
    color = "green" if is_match else "red"
    
    print(f"   –í—ñ–¥—Å—Ç–∞–Ω—å L2: {dist:.4f} (–ü–æ—Ä—ñ–≥: {threshold})")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç:   {verdict}")

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(8, 4))
    
    # –ï—Ç–∞–ª–æ–Ω
    plt.subplot(1, 2, 1)
    # Denormalize
    show_anchor = (anchor_img_tensor.permute(1, 2, 0).cpu().numpy() * 128 + 127.5) / 255.0
    plt.imshow(np.clip(show_anchor, 0, 1))
    plt.title("ANCHOR (From LFW Dataset)\nGerhard Schroeder")
    plt.axis('off')
    
    # –¢–µ—Å—Ç
    plt.subplot(1, 2, 2)
    show_test = (test_img_tensor.permute(1, 2, 0).cpu().numpy() * 128 + 127.5) / 255.0
    plt.imshow(np.clip(show_test, 0, 1))
    plt.title(f"TEST IMAGE\nDist: {dist:.3f} -> {verdict}", color=color, fontweight='bold', fontsize=9)
    plt.axis('off')
    
    save_name = f"stage2_VERIFY_{os.path.basename(test_image_path).split('.')[0]}.png"
    plt.savefig(RESULTS_DIR+'/'+save_name)
    print(f"   üñºÔ∏è –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_name}")
    plt.close()

# --- –ó–ê–ü–£–°–ö –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–Ü–í ---

test_files = [f for f in os.listdir(LOCAL_IMAGES_DIR) if f.lower().endswith(('.jpg', '.png'))]
test_files.sort()

if not test_files:
    print(f"–ù–µ–º–∞—î —Ñ–æ—Ç–æ –≤ {LOCAL_IMAGES_DIR}")
else:
    # 1. –°–ø–æ—á–∞—Ç–∫—É –ø–æ–∫–∞–∑—É—î–º–æ, —á–æ–º—É –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –ª–∞–º–∞—î—Ç—å—Å—è
    print("\n=== –ß–ê–°–¢–ò–ù–ê –ê: –ü—Ä–æ–≤–∞–ª –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞ (InceptionResnetV1 classify=True) ===")
    print("–û—á—ñ–∫—É—î–º–æ –¥—É–∂–µ –Ω–∏–∑—å–∫—É –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –∞–±–æ —Ä–∞–Ω–¥–æ–º–Ω–∏–π –∫–ª–∞—Å, –±–æ –≥–æ–ª–æ–≤–∞ –Ω–µ –Ω–∞–≤—á–µ–Ω–∞ –ø—ñ–¥ —Ü—ñ ID.")
    for f in test_files:
        demo_classification_failure(os.path.join(LOCAL_IMAGES_DIR, f), model_classifier)

    # 2. –¢–µ–ø–µ—Ä –ø–æ–∫–∞–∑—É—î–º–æ, —è–∫ —Ü–µ –≤–∏—Ä—ñ—à—É—î –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è
    print("\n=== –ß–ê–°–¢–ò–ù–ê –ë: –£—Å–ø—ñ—Ö –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (Distance Metric) ===")
    print(f"–ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –≤—Å—ñ—Ö –∑ –µ—Ç–∞–ª–æ–Ω–æ–º: {os.path.basename(anchor_path)}")
    
    for f in test_files:
        run_verification(os.path.join(LOCAL_IMAGES_DIR, f))

print("\n–í–∏—Å–Ω–æ–≤–∫–∏ –¥–ª—è –î–∏–ø–ª–æ–º—É (–ï—Ç–∞–ø 2):")
print("1. –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è 'FAIL' –ø–æ–∫–∞–∑—É—é—Ç—å, —â–æ –º–∏ –Ω–µ –º–æ–∂–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä '–∑ –∫–æ—Ä–æ–±–∫–∏'.")
print("2. –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è 'VERIFY' –¥–æ–≤–æ–¥—è—Ç—å, —â–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω–µ–π –ø—Ä–∞—Ü—é—î –Ω–∞–¥—ñ–π–Ω–æ:")
print("   - –§–æ—Ç–æ –®—Ä—å–æ–¥–µ—Ä–∞ –∑ –ø–∞–ø–∫–∏ Faces –º–∞—é—Ç—å –º–∞–ª—É –≤—ñ–¥—Å—Ç–∞–Ω—å (< 1.0) –¥–æ –µ—Ç–∞–ª–æ–Ω—É –∑ LFW.")
print("   - –§–æ—Ç–æ Jeff –º–∞—é—Ç—å –≤–µ–ª–∏–∫—É –≤—ñ–¥—Å—Ç–∞–Ω—å (> 1.0), —â–æ –¥–æ–∑–≤–æ–ª—è—î –≤—ñ–¥—Å—ñ—è—Ç–∏ –π–æ–≥–æ —è–∫ 'Unknown'.")